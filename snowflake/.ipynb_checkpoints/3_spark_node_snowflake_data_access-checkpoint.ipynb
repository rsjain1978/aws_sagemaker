{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from sagemaker_pyspark import IAMRole, classpath_jars\n",
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading snowflake jdbc driver\n",
    "\n",
    "    - Create a directory for the snowflake jar files\n",
    "    - Define the drivers to be downloaded\n",
    "    - Identify the latest version of the driver\n",
    "    - Download the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "SFC_DIR=/home/ec2-user/snowflake\n",
    "[ ! -d \"$SFC_DIR\" ] && mkdir $SFC_DIR \n",
    "cd $SFC_DIR\n",
    "PRODUCTS='snowflake-jdbc spark-snowflake_2.11'\n",
    "for PRODUCT in $PRODUCTS\n",
    "do\n",
    "   wget \"https://repo1.maven.org/maven2/net/snowflake/$PRODUCT/maven-metadata.xml\" 2> /dev/null\n",
    "   VERSION=$(grep latest maven-metadata.xml | awk -F\">\" '{ print $2 }' | awk -F\"<\" '{ print $1 }')\n",
    "   DRIVER=$PRODUCT-$VERSION.jar\n",
    "   if [[ ! -e $DRIVER ]]\n",
    "   then\n",
    "      rm $PRODUCT* 2>/dev/null\n",
    "      wget \"https://repo1.maven.org/maven2/net/snowflake/$PRODUCT/$VERSION/$DRIVER\" 2> /dev/null\n",
    "   fi\n",
    "   [ -e maven-metadata.xml ] && rm maven-metadata.xml\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 30472\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 30260258 Jul 14 17:26 snowflake-jdbc-3.12.9.jar\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   938318 Jul 14 21:45 spark-snowflake_2.11-2.8.1-spark_2.4.jar\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt /home/ec2-user/snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfc_jars=!ls -d /home/ec2-user/snowflake/*.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.2.0-1.3.1.post1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.613.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar:/home/ec2-user/snowflake/snowflake-jdbc-3.12.9.jar:/home/ec2-user/snowflake/spark-snowflake_2.11-2.8.1-spark_2.4.jar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\":\".join(classpath_jars())+\":\"+\":\".join(sfc_jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/ec2-user/snowflake/spark-snowflake_2.11-2.8.1-spark_2.4.jar,/home/ec2-user/snowflake/snowflake-jdbc-3.12.9.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=local-spark-test>\n",
      "<pyspark.sql.context.SQLContext object at 0x7fc54c273a90>\n"
     ]
    }
   ],
   "source": [
    "conf = (SparkConf()\n",
    "        #.set(\"spark.driver.extraClassPath\", (\":\".join(classpath_jars())+\":\"+\":\".join(sfc_jars)))\n",
    "        .setMaster('local')\n",
    "        .setAppName('local-spark-test'))\n",
    "sc=SparkContext.getOrCreate(conf=conf)\n",
    "sc.stop()\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "print (sc)\n",
    "\n",
    "spark = SQLContext(sc)\n",
    "print (spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://oq51261.eu-west-1.snowflakecomputing.com/\",\n",
    "  \"sfAccount\" : \"oq51261\",\n",
    "  \"sfUser\" : \"sagemaker\",\n",
    "  \"sfPassword\" : \"IRISPW\",\n",
    "  \"sfDatabase\" : \"ML_IRIS\",\n",
    "  \"sfSchema\" : \"PUBLIC\",\n",
    "  \"sfWarehouse\" : \"SAGEMAKER_WH\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "  .options(**sfOptions) \\\n",
    "  .option(\"query\",\\\n",
    "          \"\"\"select * from IRIS\"\"\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe\n",
    "iris_df = df.toPandas()\n",
    "iris_df.to_csv('iris_spark.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uplaod data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading data to s3\n",
      "data uploaded to - s3://snowflake-getting-started/iris/data/iris_spark.csv\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "s3_bucket = 'snowflake-getting-started'\n",
    "prefix = 'iris/data'\n",
    "\n",
    "print ('uploading data to s3')\n",
    "s3_data_path = session.upload_data(path='iris_spark.csv', \n",
    "                                   bucket=s3_bucket, \n",
    "                                   key_prefix=prefix)\n",
    "print ('data uploaded to -', s3_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
