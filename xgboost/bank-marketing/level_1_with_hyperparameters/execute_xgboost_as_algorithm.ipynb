{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario Description\n",
    "\n",
    "    In this notebook we are doing the following\n",
    "    - Using training & test data present in csv format\n",
    "    - doing binary classification\n",
    "    - Using a pre-built amazon container for xgboost\n",
    "    - basic hyperparamters (no tuning!)\n",
    "    - specifying debugging configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role- arn:aws:iam::951135073253:role/service-role/AmazonSageMaker-ExecutionRole-20200722T234773\n",
      "Region- eu-west-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "session = sagemaker.Session()\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print ('Role-',role)\n",
    "print ('Region-',region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment metadata would be published at - s3://snowflake-getting-started/bank-marketing/experiments-xboost\n",
      "Experiment debugging data available at - s3://snowflake-getting-started/bank-marketing/experiments-xboost/debugging\n",
      "Experiment trained moddels available at - s3://snowflake-getting-started/bank-marketing/experiments-xboost/trained_models\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "BUCKET_NAME = 'snowflake-getting-started'\n",
    "BASE_PREFIX = 'bank-marketing'\n",
    "\n",
    "EXPERIMENTS_OUTPUT_LOC = 's3://'+BUCKET_NAME+'/'+BASE_PREFIX+'/experiments-xboost'\n",
    "print ('Experiment metadata would be published at -',EXPERIMENTS_OUTPUT_LOC)\n",
    "\n",
    "EXP_DEBUGGING_OUTPUTS=EXPERIMENTS_OUTPUT_LOC+'/debugging'\n",
    "EXP_TRAINED_MODELS=EXPERIMENTS_OUTPUT_LOC+'/trained_models'\n",
    "\n",
    "print ('Experiment debugging data available at -',EXP_DEBUGGING_OUTPUTS)\n",
    "print ('Experiment trained moddels available at -',EXP_TRAINED_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"\n",
    "train_input = s3_input(\"s3://{}/{}/{}\".format(BUCKET_NAME, BASE_PREFIX, 'train/train_data.csv'), content_type=content_type)\n",
    "validation_input = s3_input(\"s3://{}/{}/{}\".format(BUCKET_NAME, BASE_PREFIX, 'test/test_data.csv'), content_type=content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "save_interval ='1'\n",
    "container = get_image_uri(region, 'xgboost',repo_version='1.0-1')\n",
    "\n",
    "print (container)\n",
    "algorithm_mode_default_estimator = Estimator(container,\n",
    "                                              train_instance_type='ml.m4.xlarge',\n",
    "                                              train_instance_count=1,\n",
    "                                              sagemaker_session = session,\n",
    "                                              role = role,\n",
    "                                              #code_location  = EXP_SOURCE_CODE,\n",
    "                                              hyperparameters = {\n",
    "                                                  'num_round':100,\n",
    "                                                  'max_depth':3,\n",
    "                                                  'eta':0.2,\n",
    "                                                  'subsample':0.8,\n",
    "                                                  \"objective\":\"binary:logistic\"\n",
    "                                              },\n",
    "                                              input_mode='File',\n",
    "                                              enable_sagemaker_metrics=True,\n",
    "                                              debugger_hook_config=DebuggerHookConfig(\n",
    "                                                          s3_output_path=EXP_DEBUGGING_OUTPUTS, \n",
    "                                                          hook_parameters={\n",
    "                                                            'save_interval': '1'\n",
    "                                                          },\n",
    "                                                          # Required - See https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#built-in-collections for supported collections\n",
    "                                                          collection_configs=[ \n",
    "                                                              CollectionConfig( name=\"metrics\"), \n",
    "                                                              CollectionConfig( name=\"feature_importance\"), \n",
    "                                                              CollectionConfig( name=\"full_shap\"), \n",
    "                                                              CollectionConfig( name=\"average_shap\"), \n",
    "                                                          ],\n",
    "                                                        ),\n",
    "                                              rules=[ \n",
    "                                                  Rule.sagemaker( \n",
    "                                                      rule_configs.loss_not_decreasing(), \n",
    "                                                      rule_parameters={ \"collection_names\": \"metrics\", \"num_steps\": str(save_interval * 2), }, \n",
    "                                                  ), \n",
    "                                              ],\n",
    "                                              output_path = EXP_TRAINED_MODELS\n",
    "                                        )\n",
    "\n",
    "\n",
    "algorithm_mode_default_estimator.fit(\n",
    "    inputs={'train': train_input, 'validation': validation_input},    \n",
    "    logs=True,\n",
    "    # This is a fire and forget event. By setting wait=False, you just submit the job to run in the background.\n",
    "    # Amazon SageMaker starts one training job and release control to next cells in the notebook.\n",
    "    # Follow this notebook to see status of the training job.\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: InProgress, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: InProgress\n",
      "Training job status: Completed, Rule Evaluation Status: Error\n",
      "Training job status: Completed, Rule Evaluation Status: Error\n",
      "Training job status: Completed, Rule Evaluation Status: Error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6a9874546fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(360):\n",
    "    job_name = algorithm_mode_default_estimator.latest_training_job.name\n",
    "    client = algorithm_mode_default_estimator.sagemaker_session.sagemaker_client\n",
    "    description = client.describe_training_job(TrainingJobName=job_name)\n",
    "    training_job_status = description[\"TrainingJobStatus\"]\n",
    "    rule_job_summary = algorithm_mode_default_estimator.latest_training_job.rule_job_summary()\n",
    "    rule_evaluation_status = rule_job_summary[0][\"RuleEvaluationStatus\"]\n",
    "    print(\"Training job status: {}, Rule Evaluation Status: {}\".format(training_job_status, rule_evaluation_status))\n",
    "\n",
    "    if rule_evaluation_status in [\"Stopped\", \"IssuesFound\", \"NoIssuesFound\"]:\n",
    "        break\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrainingJobName': 'sagemaker-xgboost-2020-07-30-19-42-26-952',\n",
       " 'TrainingJobArn': 'arn:aws:sagemaker:eu-west-1:951135073253:training-job/sagemaker-xgboost-2020-07-30-19-42-26-952',\n",
       " 'ModelArtifacts': {'S3ModelArtifacts': 's3://snowflake-getting-started/bank-marketing/experiments-xboost/trained_models/sagemaker-xgboost-2020-07-30-19-42-26-952/output/model.tar.gz'},\n",
       " 'TrainingJobStatus': 'Completed',\n",
       " 'SecondaryStatus': 'Completed',\n",
       " 'HyperParameters': {'eta': '0.2',\n",
       "  'max_depth': '3',\n",
       "  'num_round': '100',\n",
       "  'objective': 'binary:logistic',\n",
       "  'subsample': '0.8'},\n",
       " 'AlgorithmSpecification': {'TrainingImage': '141502667606.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "  'TrainingInputMode': 'File',\n",
       "  'MetricDefinitions': [{'Name': 'train:mae',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mae:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:aucpr',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-aucpr:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:merror',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-merror:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:gamma-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:mae',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mae:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:logloss',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-logloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:mlogloss',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:f1',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:accuracy',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:mse',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:poisson-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:tweedie-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:error',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-error:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:ndcg',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-ndcg:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:map',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-map:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:auc',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:gamma-deviance',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:auc',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-auc:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:error',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-error:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:merror',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-merror:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:poisson-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-poisson-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:rmse',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-rmse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:logloss',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-logloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:accuracy',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-accuracy:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:aucpr',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-aucpr:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:tweedie-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-tweedie-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:rmse',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-rmse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:gamma-deviance',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-gamma-deviance:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:mse',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mse:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:ndcg',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-ndcg:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:f1',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-f1:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:mlogloss',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-mlogloss:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'train:map',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011train-map:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'},\n",
       "   {'Name': 'validation:gamma-nloglik',\n",
       "    'Regex': '.*\\\\[[0-9]+\\\\].*#011validation-gamma-nloglik:([-+]?[0-9]*\\\\.?[0-9]+(?:[eE][-+]?[0-9]+)?).*'}],\n",
       "  'EnableSageMakerMetricsTimeSeries': True},\n",
       " 'RoleArn': 'arn:aws:iam::951135073253:role/service-role/AmazonSageMaker-ExecutionRole-20200722T234773',\n",
       " 'InputDataConfig': [{'ChannelName': 'train',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://snowflake-getting-started/bank-marketing/train/train_data.csv',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'ContentType': 'text/csv',\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'},\n",
       "  {'ChannelName': 'validation',\n",
       "   'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "     'S3Uri': 's3://snowflake-getting-started/bank-marketing/test/test_data.csv',\n",
       "     'S3DataDistributionType': 'FullyReplicated'}},\n",
       "   'ContentType': 'text/csv',\n",
       "   'CompressionType': 'None',\n",
       "   'RecordWrapperType': 'None'}],\n",
       " 'OutputDataConfig': {'KmsKeyId': '',\n",
       "  'S3OutputPath': 's3://snowflake-getting-started/bank-marketing/experiments-xboost/trained_models'},\n",
       " 'ResourceConfig': {'InstanceType': 'ml.m4.xlarge',\n",
       "  'InstanceCount': 1,\n",
       "  'VolumeSizeInGB': 30},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'CreationTime': datetime.datetime(2020, 7, 30, 19, 42, 27, 110000, tzinfo=tzlocal()),\n",
       " 'TrainingStartTime': datetime.datetime(2020, 7, 30, 19, 44, 44, 434000, tzinfo=tzlocal()),\n",
       " 'TrainingEndTime': datetime.datetime(2020, 7, 30, 19, 47, 39, 582000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 7, 30, 19, 49, 15, 195000, tzinfo=tzlocal()),\n",
       " 'SecondaryStatusTransitions': [{'Status': 'Starting',\n",
       "   'StartTime': datetime.datetime(2020, 7, 30, 19, 42, 27, 110000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 7, 30, 19, 44, 44, 434000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Preparing the instances for training'},\n",
       "  {'Status': 'Downloading',\n",
       "   'StartTime': datetime.datetime(2020, 7, 30, 19, 44, 44, 434000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 7, 30, 19, 45, 19, 449000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Downloading input data'},\n",
       "  {'Status': 'Training',\n",
       "   'StartTime': datetime.datetime(2020, 7, 30, 19, 45, 19, 449000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 7, 30, 19, 47, 31, 666000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training image download completed. Training in progress.'},\n",
       "  {'Status': 'Uploading',\n",
       "   'StartTime': datetime.datetime(2020, 7, 30, 19, 47, 31, 666000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 7, 30, 19, 47, 39, 582000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Uploading generated training model'},\n",
       "  {'Status': 'Completed',\n",
       "   'StartTime': datetime.datetime(2020, 7, 30, 19, 47, 39, 582000, tzinfo=tzlocal()),\n",
       "   'EndTime': datetime.datetime(2020, 7, 30, 19, 47, 39, 582000, tzinfo=tzlocal()),\n",
       "   'StatusMessage': 'Training job completed'}],\n",
       " 'FinalMetricDataList': [{'MetricName': 'train:error',\n",
       "   'Value': 0.0957999974489212,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 11, 22, 18, 446000, tzinfo=tzlocal())},\n",
       "  {'MetricName': 'validation:error',\n",
       "   'Value': 0.10197000205516815,\n",
       "   'Timestamp': datetime.datetime(1970, 1, 19, 11, 22, 18, 446000, tzinfo=tzlocal())}],\n",
       " 'EnableNetworkIsolation': False,\n",
       " 'EnableInterContainerTrafficEncryption': False,\n",
       " 'EnableManagedSpotTraining': False,\n",
       " 'TrainingTimeInSeconds': 175,\n",
       " 'BillableTimeInSeconds': 175,\n",
       " 'DebugHookConfig': {'S3OutputPath': 's3://snowflake-getting-started/bank-marketing/experiments-xboost/debugging',\n",
       "  'HookParameters': {'save_interval': '1'},\n",
       "  'CollectionConfigurations': [{'CollectionName': 'feature_importance'},\n",
       "   {'CollectionName': 'average_shap'},\n",
       "   {'CollectionName': 'metrics'},\n",
       "   {'CollectionName': 'losses',\n",
       "    'CollectionParameters': {'save_interval': '500'}},\n",
       "   {'CollectionName': 'full_shap'}]},\n",
       " 'DebugRuleConfigurations': [{'RuleConfigurationName': 'LossNotDecreasing',\n",
       "   'RuleEvaluatorImage': '929884845733.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "   'VolumeSizeInGB': 0,\n",
       "   'RuleParameters': {'collection_names': 'metrics',\n",
       "    'num_steps': '11',\n",
       "    'rule_to_invoke': 'LossNotDecreasing'}}],\n",
       " 'DebugRuleEvaluationStatuses': [{'RuleConfigurationName': 'LossNotDecreasing',\n",
       "   'RuleEvaluationJobArn': 'arn:aws:sagemaker:eu-west-1:951135073253:processing-job/sagemaker-xgboost-2020-07--lossnotdecreasing-1115059e',\n",
       "   'RuleEvaluationStatus': 'Error',\n",
       "   'StatusDetails': 'InternalServerError: We encountered an internal error. Please try again.',\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 30, 19, 49, 15, 188000, tzinfo=tzlocal())}],\n",
       " 'ResponseMetadata': {'RequestId': 'ee40e776-d86c-457e-82e9-a0632fe542af',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ee40e776-d86c-457e-82e9-a0632fe542af',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '8423',\n",
       "   'date': 'Thu, 30 Jul 2020 19:49:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithm_mode_default_estimator.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing and analyzing the debugger output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_mode_default_estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = algorithm_mode_default_estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "def plot_collection(trial, collection_name, regex='.*', figsize=(20, 20)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.despine()\n",
    "\n",
    "    tensors = trial.collection(collection_name).tensor_names\n",
    "\n",
    "    for tensor_name in sorted(tensors):\n",
    "        if re.match(regex, tensor_name):\n",
    "            steps, data = get_data(trial, tensor_name)\n",
    "            ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "    SUPPORTED_IMPORTANCE_TYPES = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "    if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "        raise ValueError(f\"{importance_type} is not one of the supported importance types.\")\n",
    "    plot_collection(\n",
    "        trial,\n",
    "        \"feature_importance\",\n",
    "        regex=f\"feature_importance/{importance_type}/.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial,\"average_shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
