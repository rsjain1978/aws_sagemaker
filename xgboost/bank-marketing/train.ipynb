{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.1.1-py3-none-manylinux2010_x86_64.whl (127.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 127.6 MB 15 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost) (1.18.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "def _xgb_train(params, \n",
    "               dtrain, \n",
    "               evals, \n",
    "               num_boost_round, \n",
    "               model_dir, \n",
    "               is_master):\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training, or is running single node training job. Note that rabit_run will include this argument.\n",
    "    \"\"\"\n",
    "    booster = xgb.train(params=params, dtrain=dtrain, evals=evals, num_boost_round=num_boost_round)\n",
    "\n",
    "    if is_master:\n",
    "        model_location = model_dir + '/xgboost-model'\n",
    "        pkl.dump(booster, open(model_location, 'wb'))\n",
    "        logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "        \n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--num_round', type=int, default=10)\n",
    "    parser.add_argument('--objective', type=int)\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print ('Number of rounds -', args.num_round)\n",
    "    print ('Training data location -', args.train)\n",
    "    print ('Test data location -', args.test)\n",
    "    print ('Objective -', args.objective)\n",
    "\n",
    "    dtrain = get_dmatrix(args.train, 'csv')\n",
    "    dval = get_dmatrix(args.test, 'csv')\n",
    "\n",
    "    train_hp = {\n",
    "        'num_round':args.num_round,\n",
    "        'objective':args.objective\n",
    "    }\n",
    "    \n",
    "    xgb_train_args = dict(\n",
    "        params = train_hp,\n",
    "        dtrain = dtrain,\n",
    "        evals  = dvalm,\n",
    "        model_dir=args.model_dir,        \n",
    "    )\n",
    "            \n",
    "    # If single node training, call training method directly.\n",
    "    if dtrain:\n",
    "        xgb_train_args['is_master'] = True\n",
    "        _xgb_train(**xgb_train_args)\n",
    "    else:\n",
    "        raise ValueError(\"Training channel must have data to train model.\")    \n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
